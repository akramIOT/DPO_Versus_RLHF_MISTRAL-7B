# DPO_Versus_RLHF_MISTRAL-7B
LLM Fine Tuning  ( DPO  Versus  RLHF )- Detailed Analysis and  Execution Sample with  MISTRAL7B  PreTrained Model

![image](https://github.com/akramIOT/DPO_Versus_RLHF_MISTRAL-7B/assets/21118209/6c774564-a749-4565-9515-144dd0795ca2)

# DPO_Versus_RLHF_MISTRAL-7B
LLM Fine Tuning  ( DPO  Versus  RLHF )- Detailed Analysis and  Execution Sample with  MISTRAL7B  PreTrained Model

#Reference:

https://github.com/huggingface/trl/blob/main/trl/trainer/dpo_trainer.py

![image](https://github.com/akramIOT/DPO_Versus_RLHF_MISTRAL-7B/assets/21118209/6c774564-a749-4565-9515-144dd0795ca2)


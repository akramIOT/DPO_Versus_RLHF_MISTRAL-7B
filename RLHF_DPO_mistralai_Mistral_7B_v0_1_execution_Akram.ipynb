{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akramIOT/DPO_Versus_RLHF_MISTRAL-7B/blob/main/RLHF_DPO_mistralai_Mistral_7B_v0_1_execution_Akram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aXUO6uJfqohh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning from Human Feedback\n",
        "\n",
        "RLHF basically (and I say basically rather cheekily) this process:\n",
        "\n",
        "### 1) üèÅ Start with a Model\n",
        "\n",
        "- This could be a base model, or a model that's gone through some supervised fine-tuning. For example: instruction-tuning, chat-tuning, summarization-tuning, etc.\n",
        "\n",
        "- Give the model prompts and have it generate multiple completions.\n",
        "\n",
        "### 2) üó£Ô∏è Collect Human Feedback\n",
        "\n",
        "Ask humans to evaluate these responses, selecting the preferred ones, or ranking them according to certain criteria.\n",
        "\n",
        "### 3) üí≥ Train a Reward Model\n",
        "\n",
        "- Use the collected human feedback to train a reward model.\n",
        "\n",
        "- This model learns to predict the human preference between pairs of responses.\n",
        "\n",
        "- The reward model approximates human judgments, assigning higher scores to more preferable responses.\n",
        "\n",
        "üö® **Important**: The reward model must output a scalar value score (ie, just a number) to each output generated by the LLM. This score is used for reinforcement learning, because it gives measures the output's alignment with human preferences.\n",
        "\n",
        "### 4) ü§ñ Do Reinforcement Learning\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/17574157/213037462-5dd556de-3afa-4842-b546-0fc90e799249.png\">\n",
        "\n",
        "\n",
        "- Make a duplicate copy of the language model and freeze it's weights. Now you have two models: one whose weights will be updated via back prop, and one that will not be updated. The frozen LM acts as a quality control, so the trainable LM doesn't try to cheat the system.\n",
        "\n",
        "- Generate completions from both the models. Measure how much the trainable LM's text predictions drift from the frozen one using KL divergence loss.\n",
        "\n",
        "- Add that KL divergence loss to the reward from the reward model to keep things on track. Use the trained reward model to fine-tune the base model further.\n",
        "\n",
        "- This step involves reinforcement learning algorithms, usually Proximal Policy Optimization (PPO).\n",
        "\n",
        "- Optimize the policy of the base model to generate responses that maximize the predicted reward from the reward model.\n",
        "\n",
        "\n",
        "## PPO Overview\n",
        "\n",
        "This involves three steps which, to quote Hugging Face, is as follows: Rollout, Evaluate, and Optimize.\n",
        "\n",
        "<img src=\"https://camo.githubusercontent.com/14c39f51b64e2fc3f1ed9da4af3f9027034544551aed3045d7a5930d603e233a/68747470733a2f2f68756767696e67666163652e636f2f64617461736574732f74726c2d696e7465726e616c2d74657374696e672f6578616d706c652d696d616765732f7265736f6c76652f6d61696e2f696d616765732f74726c5f6f766572766965772e706e67\">\n",
        "\n",
        "### üßª **Rollout**\n",
        "\n",
        "The language model generates completions for some prompt or query.  Basically, generate tokens that can complete the sequence.\n",
        "\n",
        "You then compute the reward for the completions and evaluate.\n",
        "\n",
        "### üî¨ **Evaluate**\n",
        "\n",
        "The query and response can be evaluated using a function, model, human feedback, or a combination.\n",
        "\n",
        "This process should produce a single numerical value for each query/response pair.\n",
        "\n",
        "You can check the scores of those tokens using the Reward Model.\n",
        "\n",
        "### üí™üèΩ **Optimize**\n",
        "\n",
        "Using a trained model and a reference model (usually the pre-trained model before fine-tuning) calculate the log probabilities of the tokens in the sequences.\n",
        "\n",
        "The KL divergence between the two outputs is then used as an additional reward signal to ensure that the generated responses do not deviate too far from the reference language model.\n",
        "\n",
        "The active language model is trained with PPO and updated based on the scores and the generations of the reference model, which is the original model before RLHF.\n",
        "\n",
        "# üéØ DPO Overview\n",
        "\n",
        "References:  https://www.summarize.tech/www.youtube.com/watch?v=pzh2oc6shic\n",
        "\n",
        "STANFORD UNIVERSOTY  SAIL: https://aibusiness.com/nlp/stanford-s-new-ai-training-method-fine-tunes-better-than-rlhf\n",
        "\n",
        "The core insight is to derive a loss function that directly solves the policy optimization and reward modeling steps simultaneously.\n",
        "\n",
        "<img src=\"https://pbs.twimg.com/media/GBGxPDTa0AEyjUd?format=jpg\">\n",
        "\n",
        "\n",
        "##  Detailed Explanation the loss function (KL LOSS and other Loss functions)\n",
        "\n",
        "References:  https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n",
        "\n",
        "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/\n",
        "\n",
        "https://arxiv.org/pdf/2305.18290.pdf\n",
        "\n",
        "https://arxiv.org/abs/2402.13228\n",
        "\n",
        "\n",
        "<img src=\"https://private-user-images.githubusercontent.com/1131538/301539636-1c4683af-7c65-4207-bd20-e2fc580a9542.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDY5MDIxMzksIm5iZiI6MTcwNjkwMTgzOSwicGF0aCI6Ii8xMTMxNTM4LzMwMTUzOTYzNi0xYzQ2ODNhZi03YzY1LTQyMDctYmQyMC1lMmZjNTgwYTk1NDIucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDIwMiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDAyMDJUMTkyMzU5WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NTlkMmI5OTQzN2U2NzA4ZjQ1ZjQ5MzQ2NjQzZjJkYTQ5YjYwNjI1OTM0NjZmMDZlMDdkZTAxNmM4YThmYzcxZSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ._b_t5lTbTsCDI_HjJUdfJUq04WgL9vbY3UNNsgVJYm0\">\n",
        "\n",
        "\n",
        "1) Loss function of policy model with respect to reference model.\n",
        "\n",
        "2) Expectation of the dataset with `x` samples of $y_{w}$ (chosen) and $y_{l}$ rejected outputs.\n",
        "\n",
        "3) The logarithmic of the sigmoid applied to the argument.\n",
        "  - In torch: `F.logsigmoid`\n",
        "  - This will scale the result between 0 and 1, providing a probabilistic interpretation.\n",
        "\n",
        "4) $\\beta$: Hyperparameter that weights the importance of the deviation between the policy model and the reference model.\n",
        "\n",
        "5) Log Probability ratio of the policy model‚Äôs probability of choosing the same  $y_{w}$  given the input `x`, divided by the reference model‚Äôs probability of choosing the same $y_{w}$  given the same input `x`.\n",
        " - These ratios indicate how much more or less likely the policy model is to choose a particular action compared to the reference model.\n",
        "\n",
        "\n",
        "### DPO Pipieline\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*AqKOT0pxzi5kOgiobb-Fvg.png\">\n",
        "\n",
        "- Start, again, with two copies of your model. One with frozen weights and one with trainable weights\n",
        "\n",
        "- Every sample gets a score from both the trainee LM and its frozen twin, based on how likely they are to spit out the expected response.\n",
        "\n",
        "- Scores for chosen versus rejected responses to a given prompt are calculated using a simple binary classification approach.\n",
        "\n",
        "- The model is trained to learn \"good\" and \"bad\" responses based on human preference data.\n",
        "\n",
        "-  The chosen/rejected score is the product of probabilities assigned to each token in the chosen response sequence. This score reflects the model's assessment of the response's quality, with higher scores indicating a response more aligned with human preferences.\n",
        "\n",
        "- The multiplication of probabilities across the sequence yields the final score for the response, which is used to guide the optimization of the model.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*-l_uEgU-Qq-32kw4DgDgxg.png\">\n",
        "\n",
        "\n",
        "- The loss on the sample in DPO is calculated using the standard cross-entropy loss function. The model produces a probability distribution over the possible responses, and the cross-entropy loss measures the difference between the predicted probability distribution and the actual distribution (the ground truth).\n",
        "\n",
        "- In the context of DPO, the actual distribution is based on human preferences, with the chosen response being the positive class and the rejected response being the negative class.\n",
        "\n",
        "- The loss is minimized when the model's predictions align closely with the human-provided labels, effectively training the model to predict human preferences with higher accuracy.\n",
        "\n",
        "##References:\n",
        "\n",
        "Direct Preference Optimization can achieve performance levels that are equivalent to, and sometimes surpass, those attainable with RLHF and PPO, as evidenced by the results of the experiments reported in Direct Preference Optimization: Your Language Model is Secretly a Reward Model.\n",
        "\n",
        "Stanford Univ and CZ BioHub, SAIL Research paper on DPO:\n",
        " https://arxiv.org/pdf/2305.18290.pdf\n",
        "\n",
        "# DPO vs RLHF\n",
        "\n",
        "\n",
        "<img src=\"https://i.ibb.co/mvFwnWc/dpo.gif\">\n",
        "\n",
        "# Akram Sheriff: Key Learnings Summary on this DPO execution project with LLM\n",
        "\n",
        "1) DPO takes a simpler execution path to aligning LLMs than RLHF technique.\n",
        "\n",
        "2) RLHF or  RLAIF  requires multiple stages of  custom data pipeline to be built in the engineering execution phase with proper selection of the Config parameters and is Not a  Trivial Task, $$$ cost and  time is involved.\n",
        "\n",
        "3) Typically RLHF pipeline involves multiple stages, including training a reward model(RM) on human feedback, using this model to generate rewards for policy training, and then using reinforcement learning algorithms to adjust the language model.\n",
        "\n",
        "4) LLM itself acts as the Secret Reward Model and hence  DPO is  good  enough technique\n",
        "\n",
        "5) RLHF or PPO is computationally intensive and requires careful hyperparameter tuning.\n",
        "\n",
        "6) The DPO pipeline directly applies a classification loss to the language model, using human preference data to guide the optimization.\n",
        "\n",
        "7) This nullifies the intricate need for optimizing of RL algorithms and the meticulous adjustment of hyperparameters. As a result, DPO runs smoother and demands less computational power than RLHF, which often requires model sampling during its fine-tuning phase and can stumble into instability and higher computational needs.\n",
        "\n",
        "8) In summary, DPO offers a streamlined and efficient alternative to the more *cumbersome* RLHF approach or  RLAIF technique.\n"
      ],
      "metadata": {
        "id": "WdBfCWWE0urf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJrcUCKs7X9S"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U datasets\n",
        "!pip install -q -U trl\n",
        "!pip install -q accelerate\n",
        "!pip install -q https://pypi.org/simple/ bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te74DElZ8r5t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from trl import DPOTrainer\n",
        "\n",
        "\n",
        "## Local MACOS M2 based Jupyter Runtime used to run this  DPO workfload and hence a Colab Widget is used to view the resources data via widget manager.\n",
        "\n",
        "#from google.colab import output\n",
        "#output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `distilabel-intel-orca-dpo-pairs dataset` is a  refinement of the [Intel/orca_dpo_pairs](https://huggingface.co/datasets/Intel/orca_dpo_pairs) through [distilabel](https://github.com/argilla-io/distilabel), aligning more closely with AI community needs and model refinement practices.\n",
        "\n",
        "Learn more about the dataset [here](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs)"
      ],
      "metadata": {
        "id": "moNfSEYBWD--"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BE7djCZ2_Qdf"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"argilla/distilabel-intel-orca-dpo-pairs\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.filter(\n",
        "    lambda r:\n",
        "        r[\"status\"] != \"tie\" and\n",
        "        r[\"chosen_score\"] >= 8 and\n",
        "        not r[\"in_gsm8k_train\"]\n",
        ")"
      ],
      "metadata": {
        "id": "v4e1VECWQuLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# You need three features in your dataset  Mandatory  for  DPO mechanism\n",
        "\n",
        " - `prompt` -  the context prompt which is given to a model at inference time for text generation\n",
        "\n",
        " - `chosen` -  the preferred generated response to the prompt\n",
        "\n",
        " - `rejected` - the dispreferred response to the prompt"
      ],
      "metadata": {
        "id": "R7WWRMTVdQm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def insert_prompts(example):\n",
        "    \"\"\"\n",
        "    Inserts data into system_prompt and user_prompt placeholders.\n",
        "\n",
        "    Args:\n",
        "        example (dict): A dictionary representing a record in the dataset.\n",
        "\n",
        "    Returns:\n",
        "        dict: The modified record with the completed prompt.\n",
        "    \"\"\"\n",
        "    system_prompt_template = \"### System:\\n{system_prompt}\\n### User:\\n{user_prompt}\\n### Assistant:\"\n",
        "    system_prompt = example[\"system\"]\n",
        "    user_prompt = example[\"input\"]\n",
        "\n",
        "\n",
        "    filled_prompt = system_prompt_template.format(system_prompt=system_prompt, user_prompt=user_prompt)\n",
        "\n",
        "    example[\"prompt\"] = filled_prompt\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(insert_prompts)"
      ],
      "metadata": {
        "id": "8WiBtMYlZuzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle(seed=42).train_test_split(seed=42, test_size=.3)"
      ],
      "metadata": {
        "id": "XdzLVzjlQuTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.remove_columns(['generations', 'order', 'labelling_model', 'labelling_prompt', 'raw_labelling_response', 'rating', 'rationale', 'status', 'original_chosen', 'original_rejected', 'chosen_score', 'in_gsm8k_train'])"
      ],
      "metadata": {
        "id": "IHxI0Qwpf5Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = dataset[\"train\"]\n",
        "\n",
        "test_data = dataset[\"test\"]"
      ],
      "metadata": {
        "id": "DUS-2ca-U8EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lChdRaiR81Dc"
      },
      "outputs": [],
      "source": [
        "'''model_name = \"Deci/DeciLM-7B-instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    use_fast=True,\n",
        "    trust_remote_code=True)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "tokenizer.pad_token_id =  tokenizer.eos_token_id\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## https://huggingface.co/mistralai/Mistral-7B-v0.1   -  Reference PreTrained Model\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    use_fast=True,\n",
        "    trust_remote_code=True)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "tokenizer.pad_token_id =  tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "57_mUMk9aR9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate policy model"
      ],
      "metadata": {
        "id": "sJvxbBZdglvX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmVQKkMI9AZQ"
      },
      "outputs": [],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "policy_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "policy_model = prepare_model_for_kbit_training(policy_model)\n",
        "\n",
        "policy_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "policy_model.config.use_cache = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqJWiy4wj1xH"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules= [\"gate_proj\", \"down_proj\", \"up_proj\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate reference model"
      ],
      "metadata": {
        "id": "0v_y7PfugqL0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nUTMKavUhF_"
      },
      "outputs": [],
      "source": [
        "reference_model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "bnb_config_ref = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "reference_model = AutoModelForCausalLM.from_pretrained(\n",
        "    reference_model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code = True,\n",
        "    low_cpu_mem_usage=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set training arguments"
      ],
      "metadata": {
        "id": "58sc5YX7sx-n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxc8sr_4-7DM"
      },
      "outputs": [],
      "source": [
        "training_arguments = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        evaluation_strategy=\"steps\",\n",
        "        do_eval=True,\n",
        "        optim=\"paged_adamw_32bit\",\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        per_device_eval_batch_size=2,\n",
        "        log_level=\"debug\",\n",
        "        save_steps=10,\n",
        "        logging_steps=10,\n",
        "        learning_rate=5e-7,\n",
        "        eval_steps=20,\n",
        "        #num_train_epochs=1,\n",
        "        max_steps=100,\n",
        "        warmup_steps=20,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        remove_unused_columns=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFnFTZkIZGmg"
      },
      "source": [
        "# Train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bu-_d4YP_CkR"
      },
      "outputs": [],
      "source": [
        "trainer = DPOTrainer(\n",
        "    policy_model,\n",
        "   # reference_model,\n",
        "    max_length=8196,\n",
        "    max_prompt_length=4096,\n",
        "    args=training_arguments,\n",
        "    beta=0.1,\n",
        "    peft_config=peft_config,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}